[
["index.html", "Lewis &amp; Clark BLT Documentation Introduction", " Lewis &amp; Clark BLT Documentation Ben Glick, Jeremy McWilliams, others! 2018-02-23 Introduction Lewis &amp; Clark was able to acquire a high performance computing cluster during the summer of 2017, as a result of Greta Binford’s capital equipment request. The cluster is a smaller version of the cluster used at Oregon State University’s Center for genome Research and Biocomputing. "],
["about-the-cluster.html", "1 About the Cluster 1.1 Cluster Facts 1.2 Interacting with the Login Node", " 1 About the Cluster 1.1 Cluster Facts The cluster has one multipurpose login node and three identical worker nodes. It has a total of 144 processing cores and 1500 gigabytes of memory. The login node also functions as the parallel filesystem for all of the workers, with 140TB of RAID-redundant disk space. This means that workers can access data stored in your home directory, which makes input and output staging extremely easy. The cluster runs the CentOS Linux operating system, version 7.4. You can interact with it as you would with any command line based linux distribution. A full list of installed software packages and modules will be posted on this wiki. 1.2 Interacting with the Login Node The only machine you should ever need to interact with is the login node, mayo. If you need help getting access to the machine, please see the Getting Connected section. When you log in to mayo, you will receive a bunch of information, including a system summary and a message from the admins, which is copied below. * This machine is for transfering files on and off the BLT computational * infrastructure. Please DO NOT run any jobs on this machine. Please login * to the server &quot;mayo.blt.lclark.edu&quot; to use the cloud and run * jobs from that machine. * * If we find jobs running on this machine the BLT Admins will kill them to ensure * other users will not be effected. * * Users are given 25G of space for free and users can check their usage * using the command &quot;quota -s&quot;. If you need more space for your project * please contact BLT Admins or IT support for details. As this message suggests, please do not run any compute-intensive jobs from the login node. It is intended to be a place to set up workflows to be run, store data files, stage data into and out of the workers, and exist as a human interaction layer so that users don’t need to deal with things like scheduler abstraction. When you log in, you will find a number of hidden files in your home directory. (If you’re curious, they can be listed with the command ls -a and will be the files that start with a dot (“.”)) Please do not remove these files as they store important information that the cluster needs access to. You will also find an empty directory called perl5. If you have specific perl libraries you need, you can install them there. If you are not using perl, you can safely ignore or delete this directory. Feel free to keep whatever data you need in your home directory. It is important to keep your directory organized in a reasonable way in order to ensure that you (and the worker nodes) will always be able to find the needed data efficiently. Also, feel free to look at what programs are installed in /local/cluster/bin, as these programs will always be able to run from any of the workers. Once you feel comfortable interacting with the login node and are ready to start to submit compute jobs, please continue to the Submitting Jobs page. "],
["getting-connected.html", "2 Getting Connected 2.1 Accounts 2.2 Getting on the network 2.3 Logging In", " 2 Getting Connected 2.1 Accounts In order to gain access to the cluster, you first need an account. Contact the BLT Admins to request an account. NOTE: Once you receive a temporary password, please reset it within 5 days of gaining access to the system. 2.2 Getting on the network The BLT cluster is quite isolated from LC’s public-facing infrastructure. In order to connect to it, you will need a copy of Cisco AnyConnect secure mobility client, which is available to LC students, faculty, and staff HERE. If you are using Linux to connect to the cluster, the current version of Cisco AnyConnect will fail to install. Luckily, there is an open-source equivalent called OpenConnect, which installs as a menu option for debian and redhat based OSes. You will need to open your network settings and click the green plus button to add a new connection, and then select VPN when prompted. After that, put in vpn.lclark.edu for the gateway option and the same root CA certificate as you used when setting up LC secure. After you click save, it will ask for your LC id and password. After you have installed and started AnyConnect: Start a VPN session by typing vpn.lclark.edu in the text box and clicking “connect” When prompted, put in your LC username and password for access Now, your computer is connected to the same virtual network as the cluster. 2.3 Logging In NOTE: In order to log in, you will need an SSH client. If you are using a Mac or Linux machine, you already have one. If you are using Windows, you will need to install PuTTY or similar. Open your SSH client On Mac Press the space bar and command key at the same time. then type “Terminal” and hit return On Linux Open a terminal window On Windows Open PuTTY Log In! On Mac or Linux type ssh &lt;lclark username&gt;@mayo.blt.lclark.edu and type your password when prompted On Windows Open PuTTY, set “Host Name” to mayo.blt.lclark.edu and click “Open”, and follow the prompt. Congratulations! You have logged in to the BLT cluster! See Using the Cluster for more information about what you can do. "],
["submitting-jobs.html", "3 Submitting Jobs 3.1 Checking Usage 3.2 A Note on Data 3.3 Jobs on the BLT Cluster", " 3 Submitting Jobs 3.1 Checking Usage At any time, a user can check what the current availability of the cluster is by typing SGE_Avail on their command line. The output will look something like this: #HOST TOTRAM FREERAM TOTSLOTS Q QSLOTS QFREESLOTS QSTATUS QTYPE bacon 503.6 500.3 48 all.q 48 48 normal BP lettuce 503.6 500.2 48 all.q 48 48 normal BP tomato 503.6 500.2 48 all.q 48 48 normal BP Right now, according to this output, there are 3 hosts running: bacon, lettuce, and tomato. They each have 48 total slots and 48 free slots. They each have 500 GB of free RAM as well. Additionally, users can check what the job queue looks like. Users can see what jobs are waiting to be run and what jobs are currently running. To do this, run the qstat command. If qstat comes back with no output, it means there are no jobs running at the moment. Here is some example output from the qstat command: job-ID prior name user state submit/start at queue slots ja-task-ID ----------------------------------------------------------------------------------------------------------------- 62 0.00000 runtime_test glick r 01/26/2018 18:59:00 1 63 0.00000 runitme_test2 glick qw 01/26/2018 18:59:02 1 64 0.00000 runtime_test3 glick qw 01/26/2018 18:59:04 1 There are currently 3 jobs on the cluster, all submitted by the user “glick”. They are jobs with ids 62,63, and 64. They each take up one slot (another name for a core). One is running, while the other two have state qw, which is short for “Queued and Waiting”. This is usually an indication that either the cluster is busy or the scheduler has not yet scheduled the jobs. 3.2 A Note on Data The home directories, /local/cluster/bin, and a few other things are mounted remotely to all of the worker nodes. This makes life easy. It means that if your script edits, reads, or otherwise depends on data from your home directory, you do not need to move the data, because the workers can access it directly. However, this also means that if your data edited by multiple jobs, there is no way to ensure that it will always be changed in the same order, so keep that in mind. 3.3 Jobs on the BLT Cluster 3.3.1 Batch Jobs A Batch job is some set of UNIX command line commands which is executed on a single core of a worker node in serial (one after another). Batch jobs can be submitted by using the following command: SGE_Batch -r &quot;&lt;some runtime id&gt;&quot; -c &quot;&lt;a UNIX command or commands&gt;&quot; 3.3.2 Parallel Jobs Parallel jobs are just like Batch jobs, except that in a parallel job, multiple cores are reserved, rather than a single core. In order to reserve multiple cores, simply add the -P flag to the SGE_Batch command like so: SGE_Batch -r &quot;&lt;runtime id&gt;&quot; -c &quot;UNIX command&quot; -P &lt;number of processors&gt; Remember that SGE_Batch will not parallelize your code for you. If your code is not meant to run on multiple cores, then using any more than 1 processor core is a waste. 3.3.3 Parsl Workflows Parsl is a python-based workflow management system that we can use to run jobs on the cluster without having to interact with the scheduler at all. They are run the same way that you would run any script on your local machine, and can orchestrate inter-process communication between almost any kind of application needed. In depth documentation about running parsl jobs is available at the Parsl Workflows page. "],
["parsl-workflows-1.html", "4 Parsl Workflows 4.1 About Parsl 4.2 Using Parsl 4.3 Parsl Configuration 4.4 Defining a Parsl Workflow 4.5 Running a Parsl Workflow 4.6 Video Tutorial", " 4 Parsl Workflows 4.1 About Parsl Parsl is a Python-based parallel scripting library that supports development and execution of asynchronous and parallel data-oriented workflows (dataflows). These workflows glue together existing executables (called Apps) and Python functions with control logic written in Python. Parsl brings implicit parallel execution to standard Python scripts. Parsl documentation is available (here)[http://parsl.readthedocs.io/en/latest/]. 4.2 Using Parsl Using parsl is a lot like using plain python. You define scripts that can be run the same way as you would normally run a python script: python3 parsl_script.py. The main difference with parsl scripts is that they don’t execute on the machine they are run from. The parsl script will provision itself resources on the worker nodes of the cluster and manage where to run jobs without any human intervention. To accomplish this, instead of returning results directly, parsl returns futures. A future is a promise that a function will return a value, usually of a specific type. The future allows parsl to have an understanding of what will be returned from all of the apps it needs to run. This helps parsl to calculate what apps can be run at the same time. 4.3 Parsl Configuration Parsl requires a configuration in order to understand what kind of an environment in which it is running. On this system, the configuration should be very similar to the example below: config = { &quot;sites&quot;: [{ &quot;site&quot;: &quot;Local_IPP&quot;, &quot;auth&quot;: { &quot;channel&quot;: &quot;local&quot; }, &quot;execution&quot;: { &quot;executor&quot;: &quot;ipp&quot;, &quot;provider&quot;: &quot;sge&quot;, &quot;script_dir&quot;: &quot;.scripts&quot;, &quot;scriptDir&quot;: &quot;.scripts&quot;, &quot;block&quot;: { &quot;nodes&quot;: 1, &quot;taskBlocks&quot;: 1, &quot;walltime&quot;: &quot;00:05:00&quot;, &quot;initBlocks&quot;: 1, &quot;minBlocks&quot;: YOUR_MIN_CORES_HERE, &quot;maxBlocks&quot;: YOUR_MAX_CORES_HERE, &quot;scriptDir&quot;: &quot;.&quot;, &quot;options&quot;: { &quot;partition&quot;: &quot;debug&quot; } } } }], &quot;globals&quot;: {&quot;lazyErrors&quot;: True}, &quot;controller&quot;: {&quot;profile&quot;: &quot;default&quot;}, } The parsl configuration is a python object, so the example can be pasted into your python script at the top, along with the following line of code: dfk = DataFlowKernel(config=config) 4.4 Defining a Parsl Workflow After that, you are ready to start defining your workflow. Here is an example workflow which calculates pi by generating random numbers: @App(&#39;python&#39;, dfk) def pi(total): # App functions have to import modules they will use. import random # Set the size of the box (edge length) in which we drop random points edge_length = 10000 center = edge_length / 2 c2 = center ** 2 count = 0 for i in range(total): # Drop a random point in the box. x, y = random.randint(1, edge_length), random.randint(1, edge_length) # Count points within the circle if (x - center)**2 + (y - center)**2 &lt; c2: count += 1 return (count * 4 / total) @App(&#39;python&#39;, dfk) def avg_n(inputs=[]): return sum(inputs) / len(inputs) if __name__ == &#39;__main__&#39;: # Call the workflow: sims = [pi(10**6) for i in range(10)] avg_pi = avg_n([task.result() for task in sims]) # Print the results print(&quot;Average: {0:.12f}&quot;.format(avg_pi.result())) You will notice a couple of things that are different from regular python. First, above each app is a decorator ( @App('python', dfk)). This tells parsl that the app should be run on a worker machine, and not as a normal python function. Second, the apps which take inputs have a special reserved parameter (inputs), which you can pass parsl futures into. Passing a future into an app will make the app wait for the result of the future before it runs the app. Another important thing to remember is that all packages you use (anything you would use the import statement for) must be imported from within the app, otherwise it will not be accessible from the worker. You can see this in the example here with the random library. 4.5 Running a Parsl Workflow After you have the proper configuration and workflow definition, you are ready to run your workflow! Make sure the workflow is in a python file on the login node (mayo) and run it like this: python3 /path/to/your/workflow/file 4.6 Video Tutorial coming soon…. "],
["setting-up-groups.html", "5 Setting up Groups 5.1 Senario for Groups 5.2 Procedures", " 5 Setting up Groups 5.1 Senario for Groups This section is for describing how to set up a group of users in which shared files might be needed for computation. Use cases include classroom/group assignments or research labs. 5.2 Procedures Connect to mayo, and navigate to /users/lab cd /users/lab Create a directory in lab. "],
["job-history.html", "A Job History", " A Job History Date: 2/9/18 Name: Ben Kolligs Department: Physics Faculty Member: Anber Description: Python script running Monte Carlo simulations of a lattice model to calculate Renyi Entropy with the hope of better understanding the strong nuclear force. Research: Funded by the NSF and Murdock Charitable Trust, we are investigating the behavior of string breaking in the strong nuclear force when the temperature is increased. Date 2/20/18 Name: Sophia Horigan &amp; Australia 2018 students Department: Biology Faculty Member: Binford Description: Introduction to bioinformatics using TransDecoder, an program in the RNA-Seq pipeline that predicts Open-Reading Frames in nucleotide sequences to identify putative amino acid coding regions. "],
["admin-reference.html", "B Admin Reference", " B Admin Reference B.0.1 Software Installations and Changes This page is intended to document all of the software installation that has been done so that we can re-do it if ever necessary. When something is installed on the cluster, you should document what it is, where it is installed, and how you installed it on this page. Note that small things like python packages do not need to be documented here. B.0.2 General Notes When you install something, please download the compressed (.tar.gz/.zip/.bz2&lt;/code&gt;…) file to /local/downloads/. Please leave uncompressed source code there too. Do not remove either of those after installing the software. B.0.3 SQLite3 We have (and must have) a slightly non-standard installation of SQLite. This just has to do with where everything is installed on our cluster’s parallel filesystem. Because of this, we need to install sqlite3 from source instead of using yum install sqlite3. To do this, I essentially followed the tutorial https://bluebill.net/2016/04/24/install-python-and-sqlite-from-source/. All of the dependencies needed to install sqlite3 should be installed already. The exact commands used (which can be rerun verbatim if needed) are below. Make sure you install python after this, because it uses header files that are only included with the distribution of SQLite3. cd /local/downloads/ wget https://www.sqlite.org/2018/sqlite-autoconf-3220000.tar.gz tar xf ./sqlite-autoconf-3220000.tar.gz cd ./sqlite-autoconf-3120200.tar.gz ./configure --prefix=/local/cluster --disable-static --enable-fts5 --enable-json1 CFLAGS=&quot;-g -O2 -DSQLITE_ENABLE_FTS3=1 -DSQLITE_ENABLE_FTS4=1 -DSQLITE_ENABLE_RTREE=1&quot; make make install B.0.4 A Note On Python Installing, configuring and managing python on a cluster like ours is frankly a mess. We have at least 5 different python interpreters installed, all of which have their own packages and package managers. If you change the configuration of any of them, please be very careful. Make sure you’re using the right ‘pip’ tool by either calling the fully-qualified path (e.g. ‘/local/cluster/bin/pip’) or by saying which pip and ensuring that the selected one is in /local/cluster/bin/. Please do not install packages or make changes to the python in /usr/bin/python. This python is important for the yum package manager. B.0.5 Python36 Because of the way they are bundled, Python3 must be installed after sqlite3 is installed. It is installed from source using the following commands, which were adapted from the same tutorial as we used to get SQLite3 (available https://bluebill.net/2016/04/24/install-python-and-sqlite-from-source/). The exact commands I ran are below: cd /local/downloads/ wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgz tar xf ./Python-3.5.1.tgz cd ./Python-3.5.1 LD_RUN_PATH=/local/cluster/lib/ LD_LIBRARY_PATH=/local/cluster/lib/ ./configure --prefix=/local/cluster LDFLAGS=&quot;-L/local/cluster/lib&quot; CPPFLAGS=&quot;-I /local/cluster/include&quot; LD_RUN_PATH=/local/cluster/lib/ LD_LIBRARY_PATH=/local/cluster/lib/ make LD_RUN_PATH=/local/cluster/lib/ LD_LIBRARY_PATH=/local/cluster/lib/ make test LD_RUN_PATH=/local/cluster/lib/ LD_LIBRARY_PATH=/local/cluster/lib/ make install Python 2 can be installed essentially identically, except with different version numbers on the things you download. B.0.6 JupyterHub Notebook Server http://jupyterhub.readthedocs.io/en/latest/ is the program we use to serve the multi-user notebook server located at mayo.blt.lclark.edu:8000. Installation of jupyterhub is easy. You can install it with ‘sudo /local/cluster/bin/pip3 install jupyterhub’. To run jupyterhub, start a screen or tmux session, run the command PATH=/local/cluster/bin/:$PATH; cd /local/cluster/jupyterhub-runtime/jupyterhub_run/ &amp;&amp; jupyterhub -f jupyterhub_config.py. Then detach the screen. Configuration is in the file /local/cluster/jupyterhub-runtime/jupyterhub_config.py I needed to make an IPTables entry allowing traffic in and out on port 8000. If you need to change the port, remember to make a new iptables entry. B.0.7 Apache Httpd Web Server We installed apache2 with the standard sudo yum install httpd. Apache2 is currently serving from its default server root at /var/www/html. PHP v5.4.16 was also installed, and has its configuration at /etc/php.ini I needed to make an IPTables entry allowing traffic in and out on port 80. If you need to change the port, remember to make a new iptables entry. B.0.8 OwnCloud OwnCloud is essentially a google drive clone that will run on our cluster. We automatically create user owncloud accounts upon user creation and we also symlink their file dropbox to their home directory. For help with installation, follow this tutorial https://www.digitalocean.com/community/tutorials/how-to-install-and-configure-owncloud-on-centos-7 B.0.9 Dropbox CLI Follow page [https://www.dropbox.com/install-linux] (Here). Our install location is somewhere else, in /local/cluster/dropbox_dist. "],
["media.html", "C Media", " C Media Theorem C.1 (Pythagorean theorem) For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] "]
]
