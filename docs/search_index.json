[
["index.html", "Lewis &amp; Clark BLT cluster About BLT", " Lewis &amp; Clark BLT cluster Watzek DI, etc. 2018-02-19 About BLT A description of BLT goes here. "],
["about-the-cluster.html", "1 About the Cluster", " 1 About the Cluster content here…. "],
["getting-connected.html", "2 Getting Connected 2.1 Accounts 2.2 Getting on the network 2.3 Logging In", " 2 Getting Connected 2.1 Accounts In order to gain access to the cluster, you first need an account. Contact the BLT Admins to request an account. NOTE: Once you receive a temporary password, please reset it within 5 days of gaining access to the system. 2.2 Getting on the network The BLT cluster is quite isolated from LC’s public-facing infrastructure. In order to connect to it, you will need a copy of Cisco AnyConnect secure mobility client, which is available to LC students, faculty, and staff HERE. If you are using Linux to connect to the cluster, the current version of Cisco AnyConnect will fail to install. Luckily, there is an open-source equivalent called OpenConnect, which installs as a menu option for debian and redhat based OSes. You will need to open your network settings and click the green plus button to add a new connection, and then select VPN when prompted. After that, put in vpn.lclark.edu for the gateway option and the same root CA certificate as you used when setting up LC secure. After you click save, it will ask for your LC id and password. After you have installed and started AnyConnect: Start a VPN session by typing vpn.lclark.edu in the text box and clicking “connect” When prompted, put in your LC username and password for access Now, your computer is connected to the same virtual network as the cluster. 2.3 Logging In NOTE: In order to log in, you will need an SSH client. If you are using a Mac or Linux machine, you already have one. If you are using Windows, you will need to install PuTTY or similar. Open your SSH client On Mac Press the space bar and command key at the same time. then type “Terminal” and hit return On Linux Open a terminal window On Windows Open PuTTY Log In! On Mac or Linux type ssh &lt;lclark username&gt;@mayo.blt.lclark.edu and type your password when prompted On Windows Open PuTTY, set “Host Name” to mayo.blt.lclark.edu and click “Open”, and follow the prompt. Congratulations! You have logged in to the BLT cluster! See Using the Cluster for more information about what you can do. "],
["submitting-jobs.html", "3 Submitting Jobs 3.1 Checking Usage 3.2 A Note on Data 3.3 Jobs on the BLT Cluster", " 3 Submitting Jobs 3.1 Checking Usage At any time, a user can check what the current availability of the cluster is by typing SGE_Avail on their command line. The output will look something like this: #HOST TOTRAM FREERAM TOTSLOTS Q QSLOTS QFREESLOTS QSTATUS QTYPE bacon 503.6 500.3 48 all.q 48 48 normal BP lettuce 503.6 500.2 48 all.q 48 48 normal BP tomato 503.6 500.2 48 all.q 48 48 normal BP Right now, according to this output, there are 3 hosts running: bacon, lettuce, and tomato. They each have 48 total slots and 48 free slots. They each have 500 GB of free RAM as well. Additionally, users can check what the job queue looks like. Users can see what jobs are waiting to be run and what jobs are currently running. To do this, run the qstat command. If qstat comes back with no output, it means there are no jobs running at the moment. Here is some example output from the qstat command: job-ID prior name user state submit/start at queue slots ja-task-ID ----------------------------------------------------------------------------------------------------------------- 62 0.00000 runtime_test glick r 01/26/2018 18:59:00 1 63 0.00000 runitme_test2 glick qw 01/26/2018 18:59:02 1 64 0.00000 runtime_test3 glick qw 01/26/2018 18:59:04 1 There are currently 3 jobs on the cluster, all submitted by the user “glick”. They are jobs with ids 62,63, and 64. They each take up one slot (another name for a core). One is running, while the other two have state qw, which is short for “Queued and Waiting”. This is usually an indication that either the cluster is busy or the scheduler has not yet scheduled the jobs. 3.2 A Note on Data The home directories, /local/cluster/bin, and a few other things are mounted remotely to all of the worker nodes. This makes life easy. It means that if your script edits, reads, or otherwise depends on data from your home directory, you do not need to move the data, because the workers can access it directly. However, this also means that if your data edited by multiple jobs, there is no way to ensure that it will always be changed in the same order, so keep that in mind. 3.3 Jobs on the BLT Cluster 3.3.1 Batch Jobs A Batch job is some set of UNIX command line commands which is executed on a single core of a worker node in serial (one after another). Batch jobs can be submitted by using the following command: SGE_Batch -r &quot;&lt;some runtime id&gt;&quot; -c &quot;&lt;a UNIX command or commands&gt;&quot; 3.3.2 Parallel Jobs Parallel jobs are just like Batch jobs, except that in a parallel job, multiple cores are reserved, rather than a single core. In order to reserve multiple cores, simply add the -P flag to the SGE_Batch command like so: SGE_Batch -r &quot;&lt;runtime id&gt;&quot; -c &quot;UNIX command&quot; -P &lt;number of processors&gt; Remember that SGE_Batch will not parallelize your code for you. If your code is not meant to run on multiple cores, then using any more than 1 processor core is a waste. 3.3.3 Parsl Workflows Parsl is a python-based workflow management system that we can use to run jobs on the cluster without having to interact with the scheduler at all. They are run the same way that you would run any script on your local machine, and can orchestrate inter-process communication between almost any kind of application needed. In depth documentation about running parsl jobs is available at the Parsl Workflows page. "]
]
